{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb4BcfUNIrpD"
      },
      "source": [
        "# <a href=\"https://miptstats.github.io/courses/ad_mipt.html\">Phystech@DataScience</a>\n",
        "## Семинар 2\n",
        "## Методы оптимизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63xX8XtTJLTf",
        "outputId": "6290e1b8-890c-479c-d1c0-253c1b3cd85a"
      },
      "outputs": [],
      "source": [
        "! apt-get install imagemagick\n",
        "! mkdir saved_gifs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq9w4zVsJPum"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image, clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, ImageMagickFileWriter\n",
        "\n",
        "sns.set(font_scale=1.6, palette='RdBu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uky7o72eI-Hp"
      },
      "source": [
        "### Оптимизаторы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV1aQmFYOIY2"
      },
      "source": [
        "Вы уже познакомились с основными методами оптимизации, которые широко используются в классическом машинном обучении. С развитием нейронных сетей и активным внедрением нейросетевого подхода, методы оптимизации стали ещё более актуальными. Но стандартные методы оптимизации имеют ряд недостатков, из-за чего их редко применяют в чистом виде. Для обучения современных нейросетей используют более продвинутые методы.\n",
        "\n",
        "Ключевая особенность всех рассматриваемых ниже методов в том, что они являются адаптивными. Т.е. для различных параметров оптимизируемой функции обновление происходит с различной скоростью.\n",
        "\n",
        "Пусть задача оптимизации имеет вид $f(x) \\longrightarrow \\min\\limits_x$, и $\\nabla_{x} f(x)$ &mdash; градиент функции $f(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiSc37vROJ1U"
      },
      "source": [
        "\n",
        "Нет универсального метода оптимизации, который всегда работает лучше, чем остальные. Поэтому для выбора наилучшего метода оптимизации и оптимальных гиперпараметров для него проводят ряд экспериментов.\n",
        "Ниже приведена визуализация нескольких экспериментов и сравнение скорости сходимости различных методов оптимизации, запущенных из одной точки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEV3UAYlOr3S"
      },
      "source": [
        "**1.SGD**\n",
        "\n",
        "Обычный и стохастический градиентный спуск.\n",
        "\n",
        "$$x_{t + 1} = x_t - \\eta v_t,$$\n",
        "\n",
        "где $v_{t} = \\nabla f(x_{t})$ &mdash; аналогия со скоростью."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf9-d9hdJR3d"
      },
      "outputs": [],
      "source": [
        "torch.optim.SGD(\n",
        "    [], # Оптимизируемые параметры\n",
        "    lr = 0.01, # Скорость обучения\n",
        "    momentum = 0.0 # уже \"следующий\" метод\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McP_YmJcO-TB"
      },
      "source": [
        "**2. SGD + Momentum**\n",
        "\n",
        "Сгладим градиент, используя информацию о том, как градиент изменялся раньше.\n",
        "Физическая аналогия &mdash; добавляем инерцию.\n",
        "\n",
        "\n",
        "$$x_{t + 1} = x_t + v_{t},$$\n",
        "где $v_{t} = \\mu v_{t - 1} - \\eta \\nabla f(x_{t})$ &mdash; сглаживаем градиенты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyovMXcaPGGX"
      },
      "outputs": [],
      "source": [
        "torch.optim.SGD(\n",
        "    [], # Оптимизируемые параметры\n",
        "    lr = 0.01, # Скорость обучения\n",
        "    momentum = 0.9 # Инертность модели, \"масса\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA4iQdxNPGtr"
      },
      "source": [
        "**3. Adagrad**\n",
        "\n",
        "Adagrad &mdash; один из самых первых адаптивных методов оптимизации.\n",
        "\n",
        "Во всех изученных ранее методах есть необходимость подбирать шаг метода (коэффициент $\\eta$). На каждой итерации все компоненты градиента оптимизируемой функции домножаются на одно и то же число $\\eta$. Но использовать одно значение $\\eta$ для всех параметров не оптимально, так как они имеют различные распределения и оптимизируемая функция изменяется с совершенно разной скоростью при небольших изменениях разных параметров.\n",
        "\n",
        "Поэтому гораздо логичнее **изменять значение каждого параметра с индивидуальной скоростью**. При этом, *чем c большей степени от изменения параметра меняется значение оптимизируемой функции, тем с меньшей скоростью стоить обновлять этот параметр*. Иначе высок шанс расходимости метода. Получить такой результат удается, если разделить градиент на сумму квадратов скорости изменений параметров.\n",
        "\n",
        "Пусть $x^{(i)}$ &mdash; $i$-я компонента вектора $x$.\n",
        "$$x_{t+1, i} = x_{t, i} - \\frac{\\eta}{\\sqrt{g_{t, i}+\\varepsilon}}\\cdot \\nabla f_i(x_t)$$\n",
        "$$g_{t} = g_{t-1} + \\nabla f(x_t) \\odot \\nabla f(x_t)$$\n",
        "\n",
        "\n",
        "В матрично-векторном виде шаг алгоритма можно переписать так:\n",
        "$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla f(x_t).$$\n",
        "Здесь $\\odot$ обозначает произведение Адамара, т.е. поэлементное перемножение векторов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnCu4D4WPOqm"
      },
      "outputs": [],
      "source": [
        "torch.optim.Adagrad(\n",
        "    [], # оптимизируемые параметры\n",
        "    lr = 0.01, # скорость обучения\n",
        "    eps = 1e-10 # epsilon задана дефолтно\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bH6so6lPPEr"
      },
      "source": [
        "**4. RMSProp**\n",
        "\n",
        "Алгоритм RMSProp основан на той же идее, что и алгоритм Adagrad &mdash; адаптировать learning rate отдельно для каждого параметра $\\theta^{(i)}$.  Однако Adagrad имеет серьёзный недостаток. Он с одинаковым весом учитывает квадраты градиентов как с самых первых итераций, так и с самых последних. Хотя, на самом деле, наибольшую значимость имеют модули градиентов на последних нескольких итерациях.\n",
        "\n",
        "Для этого предлагается использовать **экспоненциальное сглаживание**.\n",
        "$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla f(x_t).$$\n",
        "$$g_t = \\mu g_{t-1} + (1-\\mu) \\nabla f(x_t) \\odot \\nabla f(x_t)$$\n",
        "\n",
        "Стандартные значения гиперпараметров: $\\mu = 0.9, \\eta = 0.001$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBAHAwbHPQQp"
      },
      "outputs": [],
      "source": [
        "torch.optim.RMSprop(\n",
        "    [], # Оптимизируемые параметры\n",
        "    lr = 0.01, # Eta в обозначениях выше, дефолтное значение скорости обучения\n",
        "    alpha = 0.99, # Mu в обозначении выше, дефолтное значение параметра\n",
        "    eps = 1e-08 # Epsilon задана дефолтно\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rG8H7XdTAzh"
      },
      "source": [
        "**5. Adadelta**\n",
        "\n",
        "Этот метод по формуле шага и по смыслу очень похож на RMSProp. Авторы метода заметили, что в различных методах 1 порядка для оптимальной сходимости нужно брать совершенно разные значения learning rate ($\\eta$), а иногда − подбирать значение $\\eta$ в зависимости от решаемой задачи. Чтобы избавиться от необходимости находить наилучшее значение $\\eta$. Для этого корень среднеквадратичной ошибки обновления параметра (RMS) считается теперь и для $\\Delta \\theta$.\n",
        "$$d_t = \\mu d_{t-1} + (1-\\mu)\\Delta x_t \\odot \\Delta x_t$$\n",
        "$$\\Delta x_{t} = -\\frac{\\sqrt{d_{t-1}+\\varepsilon}}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla Q(x_t)$$\n",
        "$$x_{t+1} = x_t + \\Delta x_t$$\n",
        "\n",
        "Преимущество данного метода по сравнению с RMSProp − отсутствие необходимости подбирать значения параметра $\\eta$.\n",
        "Экспериментальным путём выяснено, что для Adadelta наилучшее значение $\\mu \\sim 0.9$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67c_9AeyTAkc"
      },
      "outputs": [],
      "source": [
        "torch.optim.Adadelta(\n",
        "    [], # Оптимизируемые параметры\n",
        "    lr = 1.0, # Параметр скорости обучения\n",
        "    rho = 0.9, # Mu в наших обозначениях, дефолтное значение\n",
        "    eps = 1e-06 # Epsilon, дефолтное значение\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h66O57baPe_D"
      },
      "source": [
        "**6. Adam**\n",
        "\n",
        "Этот алгоритм совмещает в себе 2 идеи:\n",
        "* идею алгоритма Momentum *о накапливании градиента*,\n",
        "* идею методов Adadelta и RMSProp *об экспоненциальном сглаживании* информации о предыдущих значениях квадратов градиентов.\n",
        "\n",
        "Благодаря использованию этих двух идей, метод имеет 2 преимущества над большей частью методов первого порядка, описанных выше:\n",
        "\n",
        "\n",
        "1. Он обновляет все параметры $\\theta$ не с одинаковым `learning rate`, а выбирает для каждого $\\theta_i$ индивидуальный `learning rate`, что *позволяет учитывать разреженные признаки с большим весом*.\n",
        "\n",
        "\n",
        "2. Adam за счёт применения экспоненциального сглаживания к градиенту *работает более устойчиво в окрестности оптимального значения параметра $\\theta^*$*, чем методы, использующие градиент в точке $x_t$, не накапливая значения градиента с прошлых шагов.\n",
        "\n",
        "\n",
        "Формулы шага алгоритма выглядят так:\n",
        "$$v_{t + 1} = \\beta v_{t} + (1-\\beta) \\nabla x(x_{t})$$\n",
        "$$g_t = \\mu g_{t-1} + (1-\\mu) \\nabla x(x_t) \\odot \\nabla x(x_t)$$\n",
        "\n",
        "Чтобы эти оценки не были смещёнными, нужно их отнормировать:\n",
        "$$\\widehat{v}_{t + 1} = \\frac{v_{t + 1}}{1-\\beta^t},$$\n",
        "$$\\widehat{g}_t = \\frac{g_t}{1-\\mu^t}.$$\n",
        "\n",
        "Тогда получим итоговую формулу шага:\n",
        "\n",
        "$$x_{t+1} = x_t - \\frac{\\eta}{\\sqrt{\\widehat{g}_t + \\varepsilon}} \\odot \\widehat{v}_{t + 1}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eazPExPsPx_V"
      },
      "outputs": [],
      "source": [
        "torch.optim.Adam(\n",
        "    [], # оптимизируемые параметры\n",
        "    lr = 0.001, # Eta в наших обозначениях, скорость обучения\n",
        "    betas = (0.9, 0.999), # Параметры mu и beta в наших обозначениях\n",
        "    eps = 1e-08 # Epsilon в наших обозначениях\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF-FfdmePzYt"
      },
      "source": [
        "**Статьи**\n",
        "\n",
        "Для того, чтобы подробнее познакомиться с представленными выше методами, мотивацией их авторов и теоретическими оценками сходимости, можно прочитать оригинальные статьи.\n",
        "\n",
        "* Adagrad: http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf,\n",
        "\n",
        "* Adadelta: https://arxiv.org/abs/1212.5701,\n",
        "\n",
        "* Adam: https://arxiv.org/abs/1412.6980.\n",
        "\n",
        "Как можно заметить, для нейросетей мы рассматриваем только методы оптимизации первого порядка. Это связано с тем, что эффективные архитектуры нейронных сетей имеют большое количество параметров, из-за чего методы второго порядка, требующие на одну итерацию $O(d^2)$ памяти и выполняющие $O(d^3)$ операций, работают слишком долго и их преимущество в количестве итераций до сходимости утрачивает смысл."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcE2uFzDbgPQ"
      },
      "source": [
        "### Эксперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93UyOgZ3bWyv"
      },
      "source": [
        "Вспомогательные функции, они нам понадобятся в дальнейшем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bwImWIbbO6N"
      },
      "outputs": [],
      "source": [
        "def make_experiment(func, trajectory, graph_title,\n",
        "                    min_y=-7, max_y=7, min_x=-7, max_x=7):\n",
        "    '''\n",
        "    Функция, которая для заданной функции рисует её линии уровня,\n",
        "    а также траекторию сходимости метода оптимизации.\n",
        "\n",
        "    Параметры.\n",
        "    1) func - оптимизируемая функция,\n",
        "    2) trajectory - траектория метода оптимизации,\n",
        "    3) graph_name - заголовок графика.\n",
        "    '''\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    xdata, ydata = [], []\n",
        "    ln, = plt.plot([], [])\n",
        "\n",
        "    mesh_x = np.linspace(min_x, max_x, 300)\n",
        "    mesh_y = np.linspace(min_y, max_y, 300)\n",
        "    X, Y = np.meshgrid(mesh_x, mesh_y)\n",
        "    Z = np.zeros((len(mesh_x), len(mesh_y)))\n",
        "\n",
        "    for coord_x in range(len(mesh_x)):\n",
        "        for coord_y in range(len(mesh_y)):\n",
        "            Z[coord_y, coord_x] = func(\n",
        "                torch.tensor((mesh_x[coord_x],\n",
        "                          mesh_y[coord_y]))\n",
        "            )\n",
        "\n",
        "    def init():\n",
        "        ax.contour(\n",
        "            X, Y, np.log(Z),\n",
        "            np.log([0.5, 10, 30, 80, 130, 200, 300, 500, 900]),\n",
        "            cmap='winter'\n",
        "        )\n",
        "        ax.set_title(graph_title)\n",
        "        return ln,\n",
        "\n",
        "    def update(frame):\n",
        "        xdata.append(trajectory[frame][0])\n",
        "        ydata.append(trajectory[frame][1])\n",
        "        ln.set_data(xdata, ydata)\n",
        "        return ln,\n",
        "\n",
        "    ani = FuncAnimation(\n",
        "        fig, update, frames=range(len(trajectory)),\n",
        "        init_func=init, repeat=True\n",
        "    )\n",
        "    writer = ImageMagickFileWriter(fps=10)\n",
        "    ani.save(f'saved_gifs/{graph_title}.gif',\n",
        "             writer=writer)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrcTp8TobPGs"
      },
      "outputs": [],
      "source": [
        "def optimize(optimizer, func_optimize, parameters, n_iter):\n",
        "    \"\"\"\n",
        "    Функция, которая для заданных функции и оптимизатора изменяет параметры,\n",
        "    сохраняя их в качестве истории вместе со значениями функциями, которая\n",
        "    оптимизируется.\n",
        "\n",
        "    Параметры.\n",
        "    1) optimizer - оптимизатор\n",
        "    2) func_optimize - оптимизируемая функция\n",
        "    3) parameters - оптимизируемые параметры\n",
        "    4) n_iter - число итераций алгоритма\n",
        "    \"\"\"\n",
        "    history = [parameters.detach().clone().numpy()]\n",
        "    losses = []\n",
        "    for _ in range(n_iter):\n",
        "      loss = func_optimize(parameters)\n",
        "      losses.append(loss.detach().clone().numpy())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      history.append(parameters.detach().clone().numpy())\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(losses)\n",
        "    return history, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJyS2GPicOGf"
      },
      "source": [
        "**SGD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIhNCIb_dB2X"
      },
      "source": [
        "Зададим параметры, оптимизатор, оптимизируемую функцию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq3bNvWWcH08"
      },
      "outputs": [],
      "source": [
        "x_init = torch.tensor([7., -7.])\n",
        "x = x_init.clone()\n",
        "x.requires_grad = True\n",
        "optimizer = torch.optim.SGD([x], lr=0.01, momentum=0.0)\n",
        "\n",
        "def func_optimize(x):\n",
        "    return 10 * x[0] ** 2 + x[1] **2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1KmB6oWdJZh"
      },
      "source": [
        "Минимизируем нашу функцию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "jduytzO8dAym",
        "outputId": "9a19006c-3e65-4b16-91a6-fa0fe9910b76"
      },
      "outputs": [],
      "source": [
        "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj_H5FaadY9Q"
      },
      "source": [
        "Посмотрим, как оно обучалось."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "PD4sfJUYdZR7",
        "outputId": "7f912a2e-5701-46f3-ea12-a1f11bf3b442"
      },
      "outputs": [],
      "source": [
        "graph_title = 'simple_test'\n",
        "make_experiment(func_optimize, history, graph_title=graph_title,\n",
        "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
        "clear_output()\n",
        "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CNQ_4-2kqBv"
      },
      "source": [
        "Более сложный пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAfswTnpkqYp"
      },
      "outputs": [],
      "source": [
        "x_init = torch.tensor([7., -7.])\n",
        "x = x_init.clone()\n",
        "x.requires_grad = True\n",
        "optimizer = torch.optim.SGD([x], lr=0.01, momentum=0.0)\n",
        "\n",
        "def func_optimize(x):\n",
        "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "-hy0BYrWkq5v",
        "outputId": "4d391cb2-826c-46c6-de03-1aad8e53b4c5"
      },
      "outputs": [],
      "source": [
        "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "SKjTYhEvkrEb",
        "outputId": "ef8fa17d-772a-4554-88a2-b994d0466d2c"
      },
      "outputs": [],
      "source": [
        "graph_title = 'other_test'\n",
        "make_experiment(func_optimize, history, graph_title=graph_title,\n",
        "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
        "clear_output()\n",
        "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agSYdDfbqX4A"
      },
      "source": [
        "По аналогии с написанным выше проведите эксперименты с остальными оптимизаторами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr3kHArndoQc"
      },
      "source": [
        "**SGD + weight**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OpwXu1yqiin"
      },
      "source": [
        "Простой пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLrrVtkwduN5"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrI7q2J5qgC6"
      },
      "source": [
        "Сложный пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "_eSc6cPpqgL7",
        "outputId": "51e7a5c2-c21d-42b9-c95f-1bab36b07a55"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L-V2dwGigw4"
      },
      "source": [
        "**Adagrad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YopXcrxzq7QB"
      },
      "source": [
        "Простой пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7HciRYDq7QB"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkJPFnoOq7QB"
      },
      "source": [
        "Сложный пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "Woq_y2V9q7QC",
        "outputId": "9d122b80-77b9-4218-f5ce-e77954716413"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uukZcseripsk"
      },
      "source": [
        "**RMSProp**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wreZdC3_rAdn"
      },
      "source": [
        "Простой пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHhTkOOorAdn"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfgUTVtfrAdn"
      },
      "source": [
        "Сложный пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "ZPfkHugerAdn",
        "outputId": "f2b47231-ee1d-480b-c6b4-1a129dfe503d"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTVuC7lqiqlu"
      },
      "source": [
        "**Adadelta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIkewkbNrCos"
      },
      "source": [
        "Простой пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv4AMwKsrCos"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4u-JC5rCos"
      },
      "source": [
        "Сложный пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "vUP9TTJBrCos",
        "outputId": "052af3e7-94b8-4e4a-846a-65ababcd1428"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3P0A10ai3kI"
      },
      "source": [
        "**Adam**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvoI3bYdrD9E"
      },
      "source": [
        "Простой пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-BP9WRPrD9E"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA5LSjqBrD9E"
      },
      "source": [
        "Сложный пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "IDeVZBDtrD9E",
        "outputId": "040d933d-1b41-4bd9-f897-67aa06ccf852"
      },
      "outputs": [],
      "source": [
        "# ваш код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiFZ7GI-i61h"
      },
      "source": [
        "**Выводы:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
